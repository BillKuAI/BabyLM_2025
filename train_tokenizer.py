import os
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace

# --- Configuration ---
# This is the segmented corpus file generated by the previous script.
INPUT_CORPUS_FILE = 'babylm_10M_morphemes.txt'

# The directory where the trained tokenizer files will be saved.
OUTPUT_TOKENIZER_DIR = 'morpheme_tokenizer'

# --- Main Tokenizer Training Script ---

def train_wordlevel_tokenizer(corpus_path, output_dir):
    """
    Trains a WordLevel tokenizer from a pre-segmented corpus file.
    """
    print(f"üöÄ Starting tokenizer training from corpus: '{corpus_path}'")

    if not os.path.exists(corpus_path):
        print(f"‚ùå Error: Corpus file not found at '{corpus_path}'.")
        print("Please make sure you have successfully run the segmentation script first.")
        return

    # 1. Initialize a new Tokenizer with the WordLevel model.
    #    The WordLevel model is ideal because it will treat each whitespace-separated
    #    string (our morphemes) as a unique token.
    #    We provide an <unk> token for any morphemes that might appear in the
    #    evaluation data but not in the training data.
    tokenizer = Tokenizer(WordLevel(unk_token="<unk>"))

    # 2. Set the pre-tokenizer.
    #    The pre-tokenizer is responsible for the initial split of the text.
    #    Using Whitespace() ensures we split exactly on the spaces between our morphemes.
    tokenizer.pre_tokenizer = Whitespace()

    # 3. Define the trainer.
    #    The WordLevelTrainer will learn the vocabulary from our corpus.
    #    We define a set of special tokens that are required by models like
    #    RoBERTa and GPT-2. This makes our single tokenizer compatible with both.
    trainer = WordLevelTrainer(
        special_tokens=[
            "<s>",      # Beginning of sentence / CLS token (for RoBERTa)
            "<pad>",    # Padding token
            "</s>",     # End of sentence / SEP token (for RoBERTa)
            "<unk>",    # Unknown token
            "<mask>",   # Mask token (for RoBERTa's MLM task)
        ]
    )

    # 4. Train the tokenizer.
    #    The trainer takes a list of files to learn from.
    print("üí™ Training tokenizer...")
    tokenizer.train([corpus_path], trainer)
    print(f"‚úÖ Training complete. Vocabulary size: {tokenizer.get_vocab_size()}")

    # 5. Save the tokenizer.
    #    The trained tokenizer is saved to a directory. Hugging Face's `from_pretrained`
    #    method can then load it directly from this directory path.
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    tokenizer.save(os.path.join(output_dir, "tokenizer.json"))
    
    print(f"üíæ Tokenizer saved to '{output_dir}' directory.")

if __name__ == '__main__':
    # Check if the input file from the previous step exists
    if os.path.exists(INPUT_CORPUS_FILE):
        train_wordlevel_tokenizer(INPUT_CORPUS_FILE, OUTPUT_TOKENIZER_DIR)
        print("\nüéâ Tokenizer is ready for use in your model training scripts!")
    else:
        print(f"---! Please run the segmentation script first to create '{INPUT_CORPUS_FILE}' !---")

